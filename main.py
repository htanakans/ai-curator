
import os, re, hashlib, json, sqlite3, datetime as dt, time
import feedparser
import requests
# --- è¿½åŠ : ãƒªãƒ³ã‚¯ç”Ÿå­˜ãƒã‚§ãƒƒã‚¯ ---
def is_alive(url: str, timeout=7) -> bool:
    if not url or not url.startswith(("http://", "https://")):
        return False
    headers = {"User-Agent": "ai-curator/1.0 (+github actions)"}
    try:
        r = requests.head(url, allow_redirects=True, timeout=timeout, headers=headers)
        if r.status_code < 400:
            return True
        # HEADã‚’æ‹’å¦ã™ã‚‹ã‚µã‚¤ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        r = requests.get(url, allow_redirects=True, timeout=timeout, headers=headers, stream=True)
        return (r.status_code < 400)
    except Exception:
        return False
from dateutil import parser as dp
from bs4 import BeautifulSoup
import polars as pl
from pathlib import Path
from feedgen.feed import FeedGenerator
import yaml

BASE = Path(__file__).parent
DATA = BASE / "data"; DATA.mkdir(exist_ok=True)
DB = DATA / "archive.sqlite3"
CFG = yaml.safe_load((BASE/"config.yml").read_text(encoding="utf-8"))

def init_db():
    con = sqlite3.connect(DB); cur = con.cursor()
    cur.execute("""CREATE TABLE IF NOT EXISTS items(
        id TEXT PRIMARY KEY,
        source TEXT, title TEXT, url TEXT, published TEXT,
        summary TEXT, tags TEXT, raw TEXT, fetched_at TEXT
    )""")
    con.commit(); con.close()

def norm_date(s):
    # å¤±æ•—ã—ã¦ã‚‚ç¾åœ¨æ™‚åˆ»ã«
    try:
        return dp.parse(s).astimezone().strftime("%Y-%m-%d %H:%M:%S%z")
    except Exception:
        return dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S%z")

def sha(s): return hashlib.sha256(s.encode("utf-8")).hexdigest()[:16]

def passes_filters(title, summary):
    text = f"{title} {summary}".lower()
    inc = [k.lower() for k in CFG.get("keywords",{}).get("include",[])]
    exc = [k.lower() for k in CFG.get("keywords",{}).get("exclude",[])]
    if inc and not any(k in text for k in inc): return False
    if any(k in text for k in exc): return False
    return True

def fetch_rss(name, url, tags):
    d = feedparser.parse(url)
    rows = []
    for e in d.entries:
        title = (e.get("title") or "").strip()
        link = (e.get("link") or "").strip()
        # è¿½åŠ : ãƒªãƒ³ã‚¯åˆ‡ã‚Œã¯ã‚¹ã‚­ãƒƒãƒ—
        if link and not is_alive(link):
            continue
        summary_html = e.get("summary") or e.get("description") or ""
        summary = BeautifulSoup(summary_html, "html.parser").get_text(" ", strip=True)
        if not title and not link: 
            continue
        if not passes_filters(title, summary): 
            continue
        uid = sha(link or title or (name+summary))
        pub = e.get("published") or e.get("updated") or ""
        rows.append(dict(
            id=uid,
            source=name,
            title=title or "(no title)",
            url=link,
            published=norm_date(pub),
            summary=summary[:2000],
            tags=",".join(tags or []),
            raw=json.dumps({k:str(v)[:10000] for k,v in e.items()}, ensure_ascii=False)
        ))
    return rows

def upsert(rows):
    con = sqlite3.connect(DB); cur = con.cursor()
    new=0
    for r in rows:
        try:
            cur.execute("INSERT OR IGNORE INTO items VALUES(?,?,?,?,?,?,?,?,datetime('now'))",
                (r["id"], r["source"], r["title"], r["url"], r["published"], r["summary"], r["tags"], r["raw"]))
            if cur.rowcount: new += 1
        except Exception as e:
            print("[WARN] upsert:", e)
    con.commit(); con.close(); 
    return new

def tidy_and_export():
    import json, csv
    con = sqlite3.connect(DB)
    cur = con.cursor()
    cur.execute("SELECT source, title, url, published, summary FROM items ORDER BY published DESC")
    rows = cur.fetchall()
    con.close()

    # æœ€æ–° 1000 ä»¶ã ã‘ã‚’å¯¾è±¡
    rows_recent = rows[:1000]

    # index.mdï¼ˆæœ€æ–°200ä»¶ï¼‰
    lines = ["# AI / ç”ŸæˆAI ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆæœ€æ–°200ä»¶ï¼‰\n"]
    for source, title, url, published, summary in rows_recent[:200]:
        date = (published or "")[:16].replace("T"," ")
        lines.append(f"- **{date}** Â· **[{title}]({url})** â€” _{source}_\n  - { (summary or '')[:160] }")
    (BASE / "index.md").write_text("\n".join(lines), encoding="utf-8")

    # JSON / CSV ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
    today = dt.datetime.now().strftime("%Y%m%d")
    dicts = [
        {"source": s, "title": t, "url": u, "published": p, "summary": (sn or "")}
        for (s, t, u, p, sn) in rows_recent
    ]
    (DATA / f"snapshot_{today}.json").write_text(
        json.dumps(dicts, ensure_ascii=False, indent=2), encoding="utf-8"
    )

    with open(DATA / f"snapshot_{today}.csv", "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=["source","title","url","published","summary"])
        w.writeheader()
        w.writerows(dicts)

    # RSS å‡ºåŠ›ï¼ˆæœ€æ–°150ä»¶ï¼‰
    fg = FeedGenerator()
    fg.title("AI/ç”ŸæˆAI ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆFree Stackï¼‰")
    fg.link(href="https://example", rel="alternate")
    fg.description("Multiple sources â†’ daily curated feed")
    fg.language("ja")
    max_items = CFG["output"].get("rss_max_items", 150)
    for item in dicts[:max_items]:
        fe = fg.add_entry()
        fe.title(item["title"] or "(no title)")
        fe.link(href=item["url"] or "")
        fe.description(item["summary"] or "")
        try:
            fe.pubDate(dp.parse(item["published"]))
        except Exception:
            pass
    fg.rss_file(DATA / "feed.xml")

def passes_local_filters(title, summary, feed_cfg):
    """ã‚µã‚¤ãƒˆå€‹åˆ¥ã® include/exclude ã‚’å„ªå…ˆã€‚ç„¡ã‘ã‚Œã°å…¨ä½“ã®keywordsã‚’ä½¿ã†ã€‚"""
    text = f"{title} {summary}".lower()
    inc = [k.lower() for k in (feed_cfg.get("include") or CFG.get("keywords",{}).get("include",[]))]
    exc = [k.lower() for k in (feed_cfg.get("exclude") or CFG.get("keywords",{}).get("exclude",[]))]
    if inc and not any(k in text for k in inc): 
        return False
    if any(k in text for k in exc): 
        return False
    return True

def fetch_site_list(feed_cfg):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’1æšã ã‘å–å¾—ã—ã¦ã€aã‚¿ã‚°ã®è¦‹å‡ºã—ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ‹¾ã†è»½é‡ã‚¯ãƒ­ãƒ¼ãƒ«"""
    url = feed_cfg["url"]
    name = feed_cfg["name"]
    try:
        r = requests.get(url, timeout=20, headers={"User-Agent":"ai-curator/1.0"})
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] fetch_site_list fail {name}: {e}")
        return []

        # --- ã“ã“ã‹ã‚‰ç½®ãæ›ãˆ ---
    # æ­£ã—ã„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ‘ãƒ¼ã‚¹ï¼ˆShift_JISç­‰ã«ã‚‚å¯¾å¿œï¼‰
    enc = r.encoding or ""
    if not enc or enc.lower() in ("iso-8859-1", "ascii"):
        enc = r.apparent_encoding or "utf-8"
        # ğŸ’¡ Mirait One å¯¾å¿œï¼šSJISã«å¼·åˆ¶å¤‰æ›ï¼ˆæ˜ç¤ºæŒ‡å®šï¼‰
    if "mirait-one.com" in url:
        enc = "cp932"
    soup = BeautifulSoup(r.content, "html.parser", from_encoding=enc)

    def _norm_text(s: str) -> str:
        s = s or ""
        # å…¨è§’ãƒ»åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚„æ”¹è¡Œã®ã ã¶ã‚Šã‚’1ã¤ã«
        return re.sub(r"\s+", " ", s).strip()

    rows = []
    for a in soup.find_all("a")[:200]:
        t = _norm_text(a.get_text(" ", strip=True))
        href = a.get("href") or ""

        if not t or not href:
            continue
        # çµ¶å¯¾URLåŒ–ï¼ˆç›¸å¯¾ãƒ‘ã‚¹å¯¾å¿œï¼‰
        if href.startswith("//"):
            href = "https:" + href
        elif href.startswith("/"):
            from urllib.parse import urljoin
            href = urljoin(url, href)
                # çµ¶å¯¾URLåŒ–ï¼ˆç›¸å¯¾ãƒ‘ã‚¹å¯¾å¿œï¼‰...ï¼ˆæ—¢å­˜ã®å‡¦ç†ã®ä¸‹ï¼‰
        if not is_alive(href):
            continue
    

        # ã‚¿ã‚¤ãƒˆãƒ«ã«å¯¾ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
        if not passes_local_filters(t, "", feed_cfg):
            continue

        rows.append(dict(
            id=sha(href or t),
            source=name,
            title=t,
            url=href,
            published=dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S%z"),
            summary="",
            tags="company,site",
            raw=""
        ))
        # å–ã‚Šã™ãé˜²æ­¢ï¼ˆå¤šãã¦ã‚‚50æœ¬ï¼‰
        if len(rows) >= 50:
            break
    return rows


def main():
    init_db()
    total_new = 0

    # é€šå¸¸ RSS
    for feed in CFG.get("feeds", []):
        name, url, tags = feed.get("name"), feed.get("url"), feed.get("tags",[])
        if not url: 
            continue
        print("[INFO] fetch", name, url)
        rows = fetch_rss(name, url, tags)
        total_new += upsert(rows)
        time.sleep(1)

    # è¿½åŠ  RSSï¼ˆNitter / RSSHub ãªã©ï¼‰
    extra = CFG.get("extra_rss") or []
    for feed in extra:
        name, url, tags = feed.get("name"), feed.get("url"), feed.get("tags",[])
        if not url: 
            continue
        print("[INFO] extra", name, url)
        rows = fetch_rss(name, url, tags)
        total_new += upsert(rows)
        time.sleep(1)

        # ä¼šç¤¾ã‚µã‚¤ãƒˆã®ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ‹¾ã„ä¸Šã’
    for s in CFG.get("site_feeds", []) or []:
        print("[INFO] site", s.get("name"), s.get("url"))
        rows = fetch_site_list(s)
        total_new += upsert(rows)

    tidy_and_export()
    print("new items:", total_new)

if __name__ == "__main__":
    main()
