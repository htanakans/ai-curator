<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI/生成AI クリッピング（Free Stack）</title><link>https://example</link><description>Multiple sources → daily curated feed</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>ja</language><lastBuildDate>Wed, 10 Sep 2025 07:44:33 +0000</lastBuildDate><item><title>No Encore: Unlearning as Opt-Out in Music Generation</title><link>https://arxiv.org/abs/2509.06277</link><description>arXiv:2509.06277v1 Announce Type: new 
Abstract: AI music generation is rapidly emerging in the creative industries, enabling intuitive music generation from textual descriptions. However, these systems pose risks in exploitation of copyrighted creations, raising ethical and legal concerns. In this paper, we present preliminary results on the first application of machine unlearning techniques from an ongoing research to prevent inadvertent usage of creative content. Particularly, we explore existing methods in machine unlearning to a pre-trained Text-to-Music (TTM) baseline and analyze their efficacy in unlearning pre-trained datasets without harming model performance. Through our experiments, we provide insights into the challenges of applying unlearning in music generation, offering a foundational analysis for future works on the application of unlearning for music generative models.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents</title><link>https://arxiv.org/abs/2509.06283</link><description>arXiv:2509.06283v1 Announce Type: cross 
Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning and tool-use capabilities has become a key focus in agentic AI research, especially with recent advances in reasoning-oriented (``thinking'') models. Such capabilities are key to unlocking a number of important applications. One such application is Deep Research (DR), which requires extensive search and reasoning over many sources. Our work in this paper focuses on the development of native Autonomous Single-Agent models for DR featuring minimal web crawling and Python tool integration. Unlike multi-agent systems, where agents take up pre-defined roles and are told what to do at each step in a static workflow, an autonomous single-agent determines its next action dynamically based on context, without manual directive. While prior work has proposed training recipes for base or instruction-tuned LLMs, we focus on continual reinforcement learning (RL) of reasoning-optimized models to further enhance agentic skills while preserving reasoning ability. Towards this end, we propose a simple RL recipe with entirely synthetic data, which we apply to various open-source LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam benchmark. In addition, we conduct key analysis experiments to provide more insights into our methodologies.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?</title><link>https://arxiv.org/abs/2509.06350</link><description>arXiv:2509.06350v1 Announce Type: new 
Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various successful methods whereby attackers manipulate models into generating harmful responses that they are designed to avoid. Among these, Greedy Coordinate Gradient (GCG) has emerged as a general and effective approach that optimizes the tokens in a suffix to generate jailbreakable prompts. While several improved variants of GCG have been proposed, they all rely on fixed-length suffixes. However, the potential redundancy within these suffixes remains unexplored. In this work, we propose Mask-GCG, a plug-and-play method that employs learnable token masking to identify impactful tokens within the suffix. Our approach increases the update probability for tokens at high-impact positions while pruning those at low-impact positions. This pruning not only reduces redundancy but also decreases the size of the gradient space, thereby lowering computational overhead and shortening the time required to achieve successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the original GCG and several improved variants. Experimental results show that most tokens in the suffix contribute significantly to attack success, and pruning a minority of low-impact tokens does not affect the loss values or compromise the attack success rate (ASR), thereby revealing token redundancy in LLM prompts. Our findings provide insights for developing efficient and interpretable LLMs from the perspective of jailbreak attacks.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>PL-CA: A Parametric Legal Case Augmentation Framework</title><link>https://arxiv.org/abs/2509.06356</link><description>arXiv:2509.06356v1 Announce Type: new 
Abstract: Conventional RAG is considered one of the most effective methods for addressing model knowledge insufficiency and hallucination, particularly in the judicial domain that requires high levels of knowledge rigor, logical consistency, and content integrity. However, the conventional RAG method only injects retrieved documents directly into the model's context, which severely constrains models due to their limited context windows and introduces additional computational overhead through excessively long contexts, thereby disrupting models' attention and degrading performance on downstream tasks. Moreover, many existing benchmarks lack expert annotation and focus solely on individual downstream tasks while real-world legal scenarios consist of multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for reflecting models' true capabilities. To address these limitations, we propose PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data augmentation on corpus knowledge and encode this legal knowledge into parametric vectors, and then integrates this parametric knowledge into the LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context pressure. Additionally, we also construct a multi-task legal dataset comprising more than 2000 training and test instances, which are all expert-annotated and manually verified. We conduct our experiments on our dataset, and the experimental results demonstrate that our method reduces the overhead associated with excessively long contexts while maintaining competitive performance on downstream tasks compared to conventional RAG. Our code and dataset are provided in the appendix.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Do LLMs exhibit the same commonsense capabilities across languages?</title><link>https://arxiv.org/abs/2509.06401</link><description>arXiv:2509.06401v1 Announce Type: new 
Abstract: This paper explores the multilingual commonsense generation abilities of Large Language Models (LLMs). To facilitate this investigation, we introduce MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. The task involves generating a commonsensical sentence that includes a given triplet of words. We evaluate a range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and Salamandra, on this benchmark. Our evaluation combines automatic metrics, LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human annotations. Results consistently show superior performance in English, with significantly lower performance in less-resourced languages. While contextual support yields mixed results, it tends to benefit underrepresented languages. These findings underscore the current limitations of LLMs in multilingual commonsense generation. The dataset is publicly available at https://huggingface.co/datasets/gplsi/MULTICOM.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models</title><link>https://arxiv.org/abs/2509.06415</link><description>arXiv:2509.06415v1 Announce Type: cross 
Abstract: Recent progress in vision-language models (VLMs) has led to impressive results in document understanding tasks, but their high computational demands remain a challenge. To mitigate the compute burdens, we propose a lightweight token pruning framework that filters out non-informative background regions from document images prior to VLM processing. A binary patch-level classifier removes non-text areas, and a max-pooling refinement step recovers fragmented text regions to enhance spatial coherence. Experiments on real-world document datasets demonstrate that our approach substantially lowers computational costs, while maintaining comparable accuracy.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents</title><link>https://arxiv.org/abs/2509.06501</link><description>arXiv:2509.06501v1 Announce Type: new 
Abstract: The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training</title><link>https://arxiv.org/abs/2509.06518</link><description>arXiv:2509.06518v1 Announce Type: new 
Abstract: Transformer-based language models traditionally use uniform (isotropic) layer sizes, yet they ignore the diverse functional roles that different depths can play and their computational capacity needs. Building on Layer-Wise Scaling (LWS) and pruning literature, we introduce three new LWS variants - Framed, Reverse, and Crown - that redistribute FFN widths and attention heads via two or three-point linear interpolation in the pre-training stage. We present the first systematic ablation of LWS and its variants, on a fixed budget of 180M parameters, trained on 5B tokens. All models converge to similar losses and achieve better performance compared to an equal-cost isotropic baseline, without a substantial decrease in training throughput. This work represents an initial step into the design space of layer-wise architectures for pre-training, but future work should scale experiments to orders of magnitude more tokens and parameters to fully assess their potential.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection</title><link>https://arxiv.org/abs/2509.06524</link><description>arXiv:2509.06524v1 Announce Type: new 
Abstract: Adapting large language models (LLMs) to specific domains often faces a critical bottleneck: the scarcity of high-quality, human-curated data. While large volumes of unchecked data are readily available, indiscriminately using them for fine-tuning risks introducing noise and degrading performance. Strategic data selection is thus crucial, requiring a method that is both accurate and efficient. Existing approaches, categorized as similarity-based and direct optimization methods, struggle to simultaneously achieve these goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for domain-specific DAta Selection), a novel approach that leverages the pre-trained LLM itself as an implicit classifier, thereby bypassing explicit feature engineering and computationally intensive optimization process. LAMDAS reframes data selection as a one-class classification problem, identifying candidate data that "belongs" to the target domain defined by a small reference dataset. Extensive experimental results demonstrate that LAMDAS not only exceeds the performance of full-data training using a fraction of the data but also outperforms nine state-of-the-art (SOTA) baselines under various scenarios. Furthermore, LAMDAS achieves the most compelling balance between performance gains and computational efficiency compared to all evaluated baselines.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion</title><link>https://arxiv.org/abs/2509.06531</link><description>arXiv:2509.06531v1 Announce Type: new 
Abstract: Link prediction in knowledge graphs requires integrating structural information and semantic context to infer missing entities. While large language models offer strong generative reasoning capabilities, their limited exploitation of structural signals often results in structural sparsity and semantic ambiguity, especially under incomplete or zero-shot settings. To address these challenges, we propose SLiNT (Structure-aware Language model with Injection and coNtrastive Training), a modular framework that injects knowledge-graph-derived structural context into a frozen LLM backbone with lightweight LoRA-based adaptation for robust link prediction. Specifically, Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive Learning (DHCL) introduces fine-grained supervision by interpolating hard positives and negatives to resolve entity-level ambiguity; and Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware intervention while preserving the core LLM parameters. Experiments on WN18RR and FB15k-237 show that SLiNT achieves superior or competitive performance compared with both embedding-based and generation-based baselines, demonstrating the effectiveness of structure-aware representation learning for scalable knowledge graph completion.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models</title><link>https://arxiv.org/abs/2509.06596</link><description>arXiv:2509.06596v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Guided Decoding and Its Critical Role in Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2509.06631</link><description>arXiv:2509.06631v1 Announce Type: new 
Abstract: The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Modelling Intertextuality with N-gram Embeddings</title><link>https://arxiv.org/abs/2509.06637</link><description>arXiv:2509.06637v1 Announce Type: new 
Abstract: Intertextuality is a central tenet in literary studies. It refers to the intricate links between literary texts that are created by various types of references. This paper proposes a new quantitative model of intertextuality to enable scalable analysis and network-based insights: perform pairwise comparisons of the embeddings of n-grams from two texts and average their results as the overall intertextuality. Validation on four texts with known degrees of intertextuality, alongside a scalability test on 267 diverse texts, demonstrates the method's effectiveness and efficiency. Network analysis further reveals centrality and community structures, affirming the approach's success in capturing and quantifying intertextual relationships.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval</title><link>https://arxiv.org/abs/2509.06650</link><description>arXiv:2509.06650v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval stage, particularly the coarse-ranking process. Existing coarse-ranking optimization approaches often struggle to balance domain-specific knowledge learning with query enhencement, resulting in suboptimal retrieval performance. To address this challenge, we propose MoLER, a domain-aware RAG method that uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of Losses (MoL) to balance domain-specific knowledge with general language capabilities, and a reinforcement learning (RL) phase leveraging Group Relative Policy Optimization (GRPO) to optimize query and passage generation for maximizing document recall. A key innovation is our Multi-query Single-passage Late Fusion (MSLF) strategy, which reduces computational overhead during RL training while maintaining scalable inference via Multi-query Multi-passage Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER achieves state-of-the-art performance, significantly outperforming baseline methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and scalable retrieval in specialized domains.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>IntrEx: A Dataset for Modeling Engagement in Educational Conversations</title><link>https://arxiv.org/abs/2509.06652</link><description>arXiv:2509.06652v1 Announce Type: new 
Abstract: Engagement and motivation are crucial for second-language acquisition, yet maintaining learner interest in educational conversations remains a challenge. While prior research has explored what makes educational texts interesting, still little is known about the linguistic features that drive engagement in conversations. To address this gap, we introduce IntrEx, the first large dataset annotated for interestingness and expected interestingness in teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus (TSCC), IntrEx extends prior work by incorporating sequence-level annotations, allowing for the study of engagement beyond isolated turns to capture how interest evolves over extended dialogues. We employ a rigorous annotation process with over 100 second-language learners, using a comparison-based rating approach inspired by reinforcement learning from human feedback (RLHF) to improve agreement. We investigate whether large language models (LLMs) can predict human interestingness judgments. We find that LLMs (7B/8B parameters) fine-tuned on interestingness ratings outperform larger proprietary models like GPT-4o, demonstrating the potential for specialised datasets to model engagement in educational settings. Finally, we analyze how linguistic and cognitive factors, such as concreteness, comprehensibility (readability), and uptake, influence engagement in educational dialogues.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data</title><link>https://arxiv.org/abs/2509.06675</link><description>arXiv:2509.06675v1 Announce Type: new 
Abstract: We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0 corpus, targeted at speech modeling tasks with the largest variant containing 2,695 hours. We combined the sound recordings of the Czech parliamentary speeches with the official transcripts. The recordings were processed with WhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our processing pipeline improves upon the ParCzech 3.0 speech recognition version by extracting more data with higher alignment reliability. The dataset is offered in three flexible variants: (1) sentence-segmented for automatic speech recognition and speech synthesis tasks with clean boundaries, (2) unsegmented preserving original utterance flow across sentences, and (3) a raw-alignment for further custom refinement for other possible tasks. All variants maintain the original metadata and are released under a permissive CC-BY license. The dataset is available in the LINDAT repository, with the sentence-segmented and unsegmented variants additionally available on Hugging Face.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments</title><link>https://arxiv.org/abs/2509.06704</link><description>arXiv:2509.06704v1 Announce Type: new 
Abstract: Aggregating multiple annotations into a single ground truth label may hide valuable insights into annotator disagreement, particularly in tasks where subjectivity plays a crucial role. In this work, we explore methods for identifying subjectivity in recognizing the human values that motivate arguments. We evaluate two main approaches: inferring subjectivity through value prediction vs. directly identifying subjectivity. Our experiments show that direct subjectivity identification significantly improves the model performance of flagging subjective arguments. Furthermore, combining contrastive loss with binary cross-entropy loss does not improve performance but reduces the dependency on per-label subjectivity. Our proposed methods can help identify arguments that individuals may interpret differently, fostering a more nuanced annotation process.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Reinforcement Learning Foundations for Deep Research Systems: A Survey</title><link>https://arxiv.org/abs/2509.06733</link><description>arXiv:2509.06733v1 Announce Type: cross 
Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction</title><link>https://arxiv.org/abs/2509.06736</link><description>arXiv:2509.06736v1 Announce Type: cross 
Abstract: Intelligent vehicle cockpits present unique challenges for API Agents, requiring coordination across tightly-coupled subsystems that exceed typical task environments' complexity. Traditional Function Calling (FC) approaches operate statelessly, requiring multiple exploratory calls to build environmental awareness before execution, leading to inefficiency and limited error recovery. We introduce VehicleWorld, the first comprehensive environment for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties with fully executable implementations that provide real-time state information during agent execution. This environment enables precise evaluation of vehicle agent behaviors across diverse, challenging scenarios. Through systematic analysis, we discovered that direct state prediction outperforms function calling for environmental control. Building on this insight, we propose State-based Function Call (SFC), a novel approach that maintains explicit system state awareness and implements direct state transitions to achieve target conditions. Experimental results demonstrate that SFC significantly outperforms traditional FC approaches, achieving superior execution accuracy and reduced latency. We have made all implementation code publicly available on Github https://github.com/OpenMOSS/VehicleWorld.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint</title><link>https://arxiv.org/abs/2509.06795</link><description>arXiv:2509.06795v1 Announce Type: new 
Abstract: Instruction Fine-Tuning (IFT) has been widely adopted as an effective post-training strategy to enhance various abilities of Large Language Models (LLMs). However, prior studies have shown that IFT can significantly compromise LLMs' safety, particularly their ability to refuse malicious instructions, raising significant concerns. Recent research into the internal mechanisms of LLMs has identified the refusal direction (r-direction) in the hidden states, which plays a pivotal role in governing refusal behavior. Building on this insight, our study reveals that the r-direction tends to drift during training, which we identify as one of the causes of the associated safety risks. To mitigate such drift, our proposed ProCon method introduces a projection-constrained loss term that regularizes the projection magnitude of each training sample's hidden state onto the r-direction. Our initial analysis shows that applying an appropriate constraint can effectively mitigate the refusal direction drift and associated safety risks, but remains limited by overall performance barriers. To overcome this barrier, informed by our observation of early-stage sharp drift and a data-driven perspective, we introduce a warm-up strategy that emphasizes early-stage strong constraints and broaden the data distribution to strengthen constraint signals, leading to an enhanced ProCon method. Experimental results under various datasets, scenarios, and LLMs demonstrate that our method can significantly mitigate safety risks posed by IFT while preserving task performance gains. Even compared with strong baselines, our method consistently delivers superior overall performance. Crucially, our analysis indicates that ProCon can contribute to stabilizing the r-direction during training, while such an interpretability-driven exploration of LLMs' internal mechanisms lays a solid foundation for future safety research.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML</title><link>https://arxiv.org/abs/2509.06806</link><description>arXiv:2509.06806v1 Announce Type: new 
Abstract: Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.
  Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.
  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security</title><link>https://arxiv.org/abs/2509.06807</link><description>arXiv:2509.06807v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) increasingly permeate human life, their security has emerged as a critical concern, particularly their ability to maintain harmless responses to malicious instructions. Although extensive methods have improved LLMs' security, they often lead to conservative, rejection-oriented responses that compromise practical usability. This presents a key challenge: how to advance the Pareto frontier between LLMs' usability and security, rather than necessitate a trade-off between them. To address this, we propose the MoGU framework, in which the intra-layer router dynamically allocates weights by sensing hidden states, thereby balancing the contributions of security-optimized and usability-optimized variants. Despite its initial potential, the MoGU framework faces limitations such as parameter redundancy and performance bottlenecks. To overcome these, we further propose an improved MoGU_v2 framework that establishes a tighter coupling between the routers and hidden states. In MoGU_v2, routers are embedded only in layers encoding highly classifiable security features, and backbone modules are activated during router optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong adaptability and stable improvements across various series of LLMs, including mainstream LLMs serving as brains in various applications, on-device LLMs optimized for resource-constrained scenarios, and reasoning LLMs tailored for user interpretability. Meanwhile, even facing risks introduced by Instruction Fine-tuning, MoGU_v2 can easily restore security without compromising the task performance gains via a simple data-mix strategy. These comprehensive improvements highlight MoGU_V2 as a robust and versatile solution for mitigating security risks in real-world applications.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem</title><link>https://arxiv.org/abs/2509.06809</link><description>arXiv:2509.06809v1 Announce Type: new 
Abstract: The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available.
  https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs</title><link>https://arxiv.org/abs/2509.06813</link><description>arXiv:2509.06813v1 Announce Type: new 
Abstract: Effective Operation and Maintenance (O&amp;M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&amp;M data quality and downstream reliability analysis.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>RAFFLES: Reasoning-based Attribution of Faults for LLM Systems</title><link>https://arxiv.org/abs/2509.06822</link><description>arXiv:2509.06822v1 Announce Type: cross 
Abstract: We have reached a critical roadblock in the development and enhancement of long-horizon, multi-component LLM agentic systems: it is incredibly tricky to identify where these systems break down and why. Evaluation capabilities that currently exist today (e.g., single pass LLM-as-a-judge) are limited in that they often focus on individual metrics or capabilities, end-to-end outcomes, and are narrowly grounded on the preferences of humans. We argue that to match the agentic capabilities, evaluation frameworks must also be able to reason, probe, iterate, and understand the complex logic passing through these systems over long horizons. In this paper, we present RAFFLES - an evaluation architecture that incorporates reasoning and iterative refinement. Specifically, RAFFLES operates as an iterative, multi-component pipeline, using a central Judge to systematically investigate faults and a set of specialized Evaluators to assess not only the system's components but also the quality of the reasoning by the Judge itself, thereby building a history of hypotheses. We tested RAFFLES against several baselines on the Who&amp;When dataset, a benchmark designed to diagnose the "who" (agent) and "when" (step) of a system's failure. RAFFLES outperforms these baselines, achieving an agent-step fault pair accuracy of over 43% on the Algorithmically-Generated dataset (a substantial increase from the previously published best of 16.6%) and over 20% on the Hand-Crafted dataset (surpassing the previously published best of 8.8%). These results demonstrate a key step towards introducing automated fault detection for autonomous systems over labor-intensive manual human review.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens</title><link>https://arxiv.org/abs/2509.06836</link><description>arXiv:2509.06836v1 Announce Type: new 
Abstract: Making LLMs more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a key technique toward this goal. However, prior pruning methods are limited: width pruning often breaks the standard transformer layout or requires custom inference code, while depth pruning removes entire layers and can cause abrupt accuracy drops. In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT enjoys merits of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN pruning), training-free operation with competitive pruning time, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream task performance at similar or higher pruning ratios, with substantial reductions in parameters, GPU memory, and end-to-end latency.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models</title><link>https://arxiv.org/abs/2509.06838</link><description>arXiv:2509.06838v1 Announce Type: new 
Abstract: Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet</title><link>https://arxiv.org/abs/2509.06861</link><description>arXiv:2509.06861v1 Announce Type: cross 
Abstract: Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>The Majority is not always right: RL training for solution aggregation</title><link>https://arxiv.org/abs/2509.06870</link><description>arXiv:2509.06870v1 Announce Type: new 
Abstract: Scaling up test-time compute, by generating multiple independent solutions and selecting or aggregating among them, has become a central paradigm for improving large language models (LLMs) on challenging reasoning tasks. While most prior work relies on simple majority voting or reward model ranking to aggregate solutions, these approaches may only yield limited benefits. In this work, we propose to learn aggregation as an explicit reasoning skill: given a set of candidate solutions, we train an aggregator model to review, reconcile, and synthesize a final, correct answer using reinforcement learning from verifiable rewards. A key ingredient is careful balancing of easy and hard training examples, allowing the model to learn both to recover minority-but-correct answers as well as easy majority-correct answers. Empirically, we find our method, AggLM, outperforms both strong rule-based and reward-model baselines, across multiple benchmarks. Furthermore, it generalizes effectively to solutions from differing models, including stronger ones than contained in the training data, all while requiring substantially fewer tokens than majority voting with larger numbers of solutions.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction</title><link>https://arxiv.org/abs/2509.06883</link><description>arXiv:2509.06883v1 Announce Type: new 
Abstract: We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>mmBERT: A Modern Multilingual Encoder with Annealed Language Learning</title><link>https://arxiv.org/abs/2509.06888</link><description>arXiv:2509.06888v1 Announce Type: new 
Abstract: Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification</title><link>https://arxiv.org/abs/2509.06902</link><description>arXiv:2509.06902v1 Announce Type: new 
Abstract: Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents</title><link>https://arxiv.org/abs/2509.06917</link><description>arXiv:2509.06917v1 Announce Type: cross 
Abstract: We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection</title><link>https://arxiv.org/abs/2509.06920</link><description>arXiv:2509.06920v1 Announce Type: cross 
Abstract: Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including precision, recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Outcome-based Exploration for LLM Reasoning</title><link>https://arxiv.org/abs/2509.06941</link><description>arXiv:2509.06941v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. We analyze this phenomenon by viewing RL post-training as a sampling process and show that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. Our study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, we propose outcome-based exploration, which assigns exploration bonuses according to final outcomes. We introduce two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, we formalize the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Interleaving Reasoning for Better Text-to-Image Generation</title><link>https://arxiv.org/abs/2509.06945</link><description>arXiv:2509.06945v1 Announce Type: cross 
Abstract: Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning</title><link>https://arxiv.org/abs/2509.06948</link><description>arXiv:2509.06948v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has proven effective in incentivizing the reasoning abilities of large language models (LLMs), but suffers from severe efficiency challenges due to its trial-and-error nature. While the common practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this decoupled two-stage approach limits interaction between SFT and RL, thereby constraining overall effectiveness. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. By conditioning the SFT objective on the optimal RL policy, our approach enables SFT to meta-learn how to guide RL's optimization process. During training, the lower level performs RL updates while simultaneously receiving SFT supervision, and the upper level explicitly maximizes the cooperative gain-the performance advantage of joint SFT-RL training over RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models</title><link>https://arxiv.org/abs/2509.06949</link><description>arXiv:2509.06949v1 Announce Type: new 
Abstract: We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts</title><link>https://arxiv.org/abs/2509.06952</link><description>arXiv:2509.06952v1 Announce Type: new 
Abstract: Language use is shaped by pragmatics -- i.e., reasoning about communicative goals and norms in context. As language models (LMs) are increasingly used as conversational agents, it becomes ever more important to understand their pragmatic reasoning abilities. We propose an evaluation framework derived from Wavelength, a popular communication game where a speaker and a listener communicate about a broad range of concepts in a granular manner. We study a range of LMs on both language comprehension and language production using direct and Chain-of-Thought (CoT) prompting, and further explore a Rational Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM inference. We find that state-of-the-art LMs, but not smaller ones, achieve strong performance on language comprehension, obtaining similar-to-human accuracy and exhibiting high correlations with human judgments even without CoT prompting or RSA. On language production, CoT can outperform direct prompting, and using RSA provides significant improvements over both approaches. Our study helps identify the strengths and limitations in LMs' pragmatic reasoning abilities and demonstrates the potential for improving them with RSA, opening up future avenues for understanding conceptual representation, language understanding, and social reasoning in LMs and humans.</description><pubDate>Tue, 09 Sep 2025 04:00:00 +0000</pubDate></item><item><title>Help! My therapist is secretly using ChatGPT</title><link>https://www.technologyreview.com/2025/09/09/1123386/help-my-therapist-is-secretly-using-chatgpt/</link><description>In Silicon Valley’s imagined future, AI models are so empathetic that we’ll use them as therapists. They’ll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.  Last week, we published a…</description><pubDate>Tue, 09 Sep 2025 09:00:00 +0000</pubDate></item><item><title>AI is changing the grid. Could it help more than it harms?</title><link>https://www.technologyreview.com/2025/09/09/1123404/ai-grid-help/</link><description>The rising popularity of AI is driving an increase in electricity demand so significant it has the potential to reshape our grid. Energy consumption by data centers has gone up by 80% from 2020 to 2025 and is likely to keep growing. Electricity prices are already rising, especially in places where data centers are most…</description><pubDate>Tue, 09 Sep 2025 09:00:00 +0000</pubDate></item><item><title>Three big things we still don’t know about AI’s energy burden</title><link>https://www.technologyreview.com/2025/09/09/1123408/three-big-things-we-still-dont-know-about-ais-energy-burden/</link><description>Earlier this year, when my colleague Casey Crownhart and I spent six months researching the climate and energy burden of AI, we came to see one number in particular as our white whale: how much energy the leading AI models, like ChatGPT or Gemini, use up when generating a single response.  This fundamental number remained…</description><pubDate>Tue, 09 Sep 2025 09:00:00 +0000</pubDate></item><item><title>The Download: meet our AI innovators, and what happens when therapists use AI covertly</title><link>https://www.technologyreview.com/2025/09/09/1123447/the-download-meet-our-ai-innovators-and-what-happens-when-therapists-use-ai-covertly/</link><description>This is today’s edition of The Download, our weekday newsletter that provides a daily dose of what’s going on in the world of technology. Meet the AI honorees on our 35 Innovators Under 35 list for 2025 Each year, we select 35 outstanding individuals under the age of 35 who are using technology to tackle tough problems…</description><pubDate>Tue, 09 Sep 2025 12:10:00 +0000</pubDate></item><item><title>NVIDIA Blackwell Ultra Sets New Inference Records in MLPerf Debut</title><link>https://developer.nvidia.com/blog/nvidia-blackwell-ultra-sets-new-inference-records-in-mlperf-debut/</link><description>As large language models (LLMs) grow larger, they get smarter, with open models from leading developers now featuring hundreds of billions of parameters. At the...</description><pubDate>Tue, 09 Sep 2025 15:00:00 +0000</pubDate></item><item><title>NVIDIA Rubin CPX Accelerates Inference Performance and Efficiency for 1M+ Token Context Workloads</title><link>https://developer.nvidia.com/blog/nvidia-rubin-cpx-accelerates-inference-performance-and-efficiency-for-1m-token-context-workloads/</link><description>Inference has emerged as the new frontier of complexity in AI. Modern models are evolving into agentic systems capable of multi-step reasoning, persistent...</description><pubDate>Tue, 09 Sep 2025 15:00:00 +0000</pubDate></item><item><title>Adapting to new threats with proactive risk management</title><link>https://www.technologyreview.com/2025/09/09/1123083/adapting-to-new-threats-with-proactive-risk-management/</link><description>In July 2024, a botched update to the software defenses managed by cybersecurity firm CrowdStrike caused more than 8 million Windows systems to fail. From hospitals to manufacturers, stock markets to retail stores, the outage caused parts of the global economy to grind to a halt. Payment systems were disrupted, broadcasters went off the air,…</description><pubDate>Tue, 09 Sep 2025 15:00:00 +0000</pubDate></item><item><title>How to Connect Distributed Data Centers Into Large AI Factories with Scale-Across Networking</title><link>https://developer.nvidia.com/blog/how-to-connect-distributed-data-centers-into-large-ai-factories-with-scale-across-networking/</link><description>AI scaling is incredibly complex, and new techniques in training and inference are continually demanding more out of the data center. While data center...</description><pubDate>Tue, 09 Sep 2025 17:00:00 +0000</pubDate></item><item><title>Skip to main content</title><link>#main-content</link></item><item><title>Try Claude</title><link>https://claude.ai/</link></item><item><title>Le Chat. Custom MCP connectors. Memories. Product September 2, 2025 Mistral AI Le Chat now integrates with 20+ enterprise platforms—powered by MCP—and remembers what matters with Memories.</title><link>https://mistral.ai/news/le-chat-mcp-connectors-memories</link></item><item><title>AI solutions</title><link>https://mistral.ai/solutions</link></item><item><title>Product Claude Sonnet 4 now supports 1M tokens of context Aug 12, 2025</title><link>https://www.anthropic.com/news/1m-context</link></item><item><title>Event Introducing Anthropic's first developer conference: Code with Claude Apr 3, 2025</title><link>https://www.anthropic.com/news/Introducing-code-with-claude</link></item><item><title>Policy Activating AI Safety Level 3 protections May 22, 2025</title><link>https://www.anthropic.com/news/activating-asl3-protections</link></item><item><title>Product Advancing Claude for Education Jul 09, 2025</title><link>https://www.anthropic.com/news/advancing-claude-for-education</link></item><item><title>Announcements Introducing Anthropic's AI for Science Program May 05, 2025</title><link>https://www.anthropic.com/news/ai-for-science-program</link></item><item><title>Announcements Anthropic and the Department of Defense to advance responsible AI in defense operations Jul 14, 2025</title><link>https://www.anthropic.com/news/anthropic-and-the-department-of-defense-to-advance-responsible-ai-in-defense-operations</link></item><item><title>Societal Impacts Anthropic Education Report: How educators use Claude Aug 27, 2025</title><link>https://www.anthropic.com/news/anthropic-education-report-how-educators-use-claude</link></item><item><title>Announcements Anthropic Education Report: How university students use Claude Apr 08, 2025</title><link>https://www.anthropic.com/news/anthropic-education-report-how-university-students-use-claude</link></item><item><title>Announcements Anthropic launches higher education advisory board and AI Fluency courses Aug 21, 2025</title><link>https://www.anthropic.com/news/anthropic-higher-education-initiatives</link></item><item><title>Announcements Anthropic partners with the University of Chicago’s Becker Friedman Institute on AI economic research Jul 23, 2025</title><link>https://www.anthropic.com/news/anthropic-partners-with-the-university-of-chicago-s-becker-friedman-institute-on-ai-economic</link></item><item><title>Announcements Anthropic raises $13B Series F at $183B post-money valuation Sep 02, 2025</title><link>https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation</link></item><item><title>Announcements Anthropic Signs White House Pledge to America's Youth: Investing in AI Education Sep 04, 2025</title><link>https://www.anthropic.com/news/anthropic-signs-pledge-to-americas-youth-investing-in-ai-education</link></item><item><title>Announcements Automate security reviews with Claude Code Aug 6, 2025</title><link>https://www.anthropic.com/news/automate-security-reviews-with-claude-code</link></item><item><title>Policy Build AI in America Jul 21, 2025</title><link>https://www.anthropic.com/news/build-ai-in-america</link></item><item><title>Product Turn ideas into interactive AI-powered apps Jun 25, 2025</title><link>https://www.anthropic.com/news/build-artifacts</link></item><item><title>Product Building safeguards for Claude Aug 12, 2025</title><link>https://www.anthropic.com/news/building-safeguards-for-claude</link></item><item><title>Announcements Introducing Claude 4 May 22, 2025</title><link>https://www.anthropic.com/news/claude-4</link></item><item><title>Product Claude Code and new admin controls for business plans Aug 20, 2025</title><link>https://www.anthropic.com/news/claude-code-on-team-and-enterprise</link></item><item><title>Product Remote MCP support in Claude Code Jun 18, 2025</title><link>https://www.anthropic.com/news/claude-code-remote-mcp</link></item><item><title>Product Piloting Claude for Chrome Aug 26, 2025</title><link>https://www.anthropic.com/news/claude-for-chrome</link></item><item><title>Product Claude for Financial Services Helping finance professionals analyze markets, conduct research, and make investment decisions. Jul 15, 2025</title><link>https://www.anthropic.com/news/claude-for-financial-services</link></item><item><title>Announcements Claude Gov models for U.S. national security customers Jun 06, 2025</title><link>https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers</link></item><item><title>Announcements Claude in Amazon Bedrock: Approved for use in FedRAMP High and DoD IL4/5 workloads Jun 11, 2025</title><link>https://www.anthropic.com/news/claude-in-amazon-bedrock-fedramp-high</link></item><item><title>Product Claude on Google Cloud’s Vertex AI: FedRAMP High and IL2 Authorized Apr 02, 2025</title><link>https://www.anthropic.com/news/claude-on-google-cloud-fedramp-high</link></item><item><title>Announcements Claude Opus 4.1 Our most powerful model for handling complex agent and coding tasks Aug 05, 2025</title><link>https://www.anthropic.com/news/claude-opus-4-1</link></item><item><title>Product Build and share AI-powered apps with Claude Jun 25, 2025</title><link>https://www.anthropic.com/news/claude-powered-artifacts</link></item><item><title>Product Discover tools that work with Claude Jul 14, 2025</title><link>https://www.anthropic.com/news/connectors-directory</link></item><item><title>Product Claude can now create and edit files Sep 09, 2025</title><link>https://www.anthropic.com/news/create-files</link></item><item><title>Societal Impacts Detecting and countering malicious uses of Claude: March 2025 Apr 23, 2025</title><link>https://www.anthropic.com/news/detecting-and-countering-malicious-uses-of-claude-march-2025</link></item><item><title>Announcements Detecting and countering misuse of AI: August 2025 Aug 27, 2025</title><link>https://www.anthropic.com/news/detecting-countering-misuse-aug-2025</link></item><item><title>Announcements Developing nuclear safeguards for AI through public-private partnership Aug 21, 2025</title><link>https://www.anthropic.com/news/developing-nuclear-safeguards-for-ai-through-public-private-partnership</link></item><item><title>Announcements Federal government departments and agencies can now purchase Claude through the GSA schedule Aug 05, 2025</title><link>https://www.anthropic.com/news/federal-government-departments-and-agencies-can-now-purchase-claude-through-the-gsa-schedule</link></item><item><title>Case Study How Anthropic teams use Claude Code Jul 24, 2025</title><link>https://www.anthropic.com/news/how-anthropic-teams-use-claude-code</link></item><item><title>How people use Claude for support, advice, and companionship Jun 27, 2025</title><link>https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship</link></item><item><title>Product Claude can now connect to your world May 01, 2025</title><link>https://www.anthropic.com/news/integrations</link></item><item><title>Alignment Investing in energy to secure America's AI future Jul 15, 2025</title><link>https://www.anthropic.com/news/investing-in-energy-to-secure-america-s-ai-future</link></item><item><title>Announcements Lawrence Livermore National Laboratory expands Claude for Enterprise use to empower scientists and researchers Jul 09, 2025</title><link>https://www.anthropic.com/news/lawrence-livermore-national-laboratory-expands-claude-for-enterprise-to-empower-scientists-and</link></item><item><title>Announcements National security expert Richard Fontaine appointed to Anthropic’s long-term benefit trust Jun 7, 2025</title><link>https://www.anthropic.com/news/national-security-expert-richard-fontaine-appointed-to-anthropic-s-long-term-benefit-trust</link></item><item><title>Announcements Offering expanded Claude access across all three branches of the U.S. government Aug 12, 2025</title><link>https://www.anthropic.com/news/offering-expanded-claude-access-across-all-three-branches-of-government</link></item><item><title>Policy Our approach to understanding and addressing AI harms Apr 21, 2025</title><link>https://www.anthropic.com/news/our-approach-to-understanding-and-addressing-ai-harms</link></item><item><title>Product Claude takes research to new places Apr 15, 2025</title><link>https://www.anthropic.com/news/research</link></item><item><title>Policy The need for transparency in Frontier AI Jul 07, 2025</title><link>https://www.anthropic.com/news/the-need-for-transparency-in-frontier-ai</link></item><item><title>Policy Thoughts on America’s AI Action Plan Jul 23, 2025</title><link>https://www.anthropic.com/news/thoughts-on-america-s-ai-action-plan</link></item><item><title>Societal Impacts Anthropic Economic Index: AI’s impact on software development Apr 28, 2025</title><link>https://www.anthropic.com/research/impact-software-development</link></item><item><title>September  5, 2025 | Sony AI Advancing AI: Highlights from August</title><link>https://ai.sony/blog/Advancing-AI-Highlights-from-August/</link></item><item><title>September  9, 2025 | Sony AI Advancing Analog Design with AI: Sony AIâs Contributions at MLCAD 2025</title><link>https://ai.sony/blog/Advancing-Analog-Design-with-AI-Sony-AIâs-Contributions-at-MLCAD-2025/</link></item><item><title>August 26, 2025 | Game AI Gaming Life at Sony AI Robotics Sony AI Sony AIâs Deep RL Team on Why the Hardest Problems Still Matter</title><link>https://ai.sony/blog/Sony-AIâs-Deep-RL-Team-on-Why-the-Hardest-Problems-Still-Matter/</link></item><item><title>Borderless AI</title><link>https://cohere.com/customer-stories/borderless-ai</link></item><item><title>LLM University</title><link>https://cohere.com/llmu</link></item><item><title>North NEW An enterprise-ready AI platform that powers modern workplace productivity</title><link>https://cohere.com/north</link></item><item><title>Security Best-in-class AI security and data protection</title><link>https://cohere.com/security</link></item><item><title>People Learn more about Sony AI and the type of people we are looking for to join our team</title><link>https://www.youtube.com/watch?v=NhaDzRmyTHg</link></item><item><title>The Team Behind GT Sophy - Part2 Sony AI is proud to share the second installment of our new video series, âThe Team Behind GT Sophy.â
                  In this episode
                  the team at Sony AI take the racing agent they have trained to play âGran Turismoâ and put it the
                  ultimate test. At the
                  April 2021 Race Together event, GT Sophy would face off against some of the finest human âGran
                  Turismoâ players in the
                  world. Had the team taught their AI racer to master the skills required to hold their own against the
                  champions of
                  esports?</title><link>https://www.youtube.com/watch?v=hsp7v5FC_6s</link></item><item><title>The Team Behind GT Sophy - Part3 All of the training and preparation comes to a head in the third and final installment of Sony AIâs new video series,
                âThe Team Behind GT Sophy.â In this episode, the AI racing agent GT Sophy gets a rematch against the worldâs best
                esports champions in a rematch after falling short in its initial face off.</title><link>https://www.youtube.com/watch?v=zdMz-lDh-QE</link></item><item><title>大規模複合施設「HANEDA INNOVATION CITY ® 」グランドオープン1 周年記念イベント「あわい-awai 2024-」開催決定</title><link>202409/pdf/27a1-j.pdf</link></item><item><title>大規模複合施設「HANEDA INNOVATION CITY ® 」グランドオープン1 周年記念イベント「あわい-awai 2024-」詳細決定</title><link>202410/pdf/18a1-j.pdf</link></item><item><title>柱一本を全自動で溶接　新型のマニピュレータ型現場溶接ロボットを実導入</title><link>202411/11a1-j.htm</link></item><item><title>CSGの締固め品質管理手法「Geo-DX Compaction TM 」を成瀬ダムに導入し、試験要員を7割削減</title><link>202412/19c1-j.htm</link></item><item><title>自動化施工システム「A 4 CSEL」　造成工事への本格適用を開始</title><link>202412/24c1-j.htm</link></item><item><title>資機材自動搬送ロボットを開発</title><link>202412/pdf/17a1-j.pdf</link></item><item><title>車や人との衝突を未然に防ぐ　道路横断におけるロボットの自動制御に成功</title><link>202501/10a1-j.htm</link></item><item><title>羽田イノベーションシティにて到達範囲の広い電波規格「Wi-SUN FAN」によるロボット遠隔誘導の実証実験に成功</title><link>202501/22a1-j.htm</link></item><item><title>道路橋の床版更新における設計業務時間を10分の1に　3Dモデル自動生成システムを開発</title><link>202502/27c1-j.htm</link></item><item><title>自動化施工システム「A 4 CSEL ® 」の普及展開を見据え、建設会社との連携を試行</title><link>202502/6c1-j.htm</link></item><item><title>自動化施工システム「A 4 CSEL」　4機種連携により盛土の一連作業を自動化</title><link>202506/20c1-j.htm</link></item><item><title>2025.06.20 Python×AI×交通ナビで社会貢献！SmoothNav開発ストーリー 〜未経験から始めるAIエンジニアへの道〜 エンジニアのための情報共有コミュニティ「Zenn」にて配信されました。社会に役立つ交通ナビの開発体験を、未経験からAIエンジニアを目指す方に向けてご紹介します。</title><link>https://renue.co.jp/posts/AI-engineer</link></item><item><title>2025.06.17 AIと創る鉄道の未来：東海道新幹線運行シミュレーション開発の全貌とAIとのペアプログラミング道 エンジニアのための情報共有コミュニティ「Zenn」にて配信されました。AI搭載開発ツールCursorとペアプログラミングを行う上での対話術についてご紹介します。</title><link>https://renue.co.jp/posts/Cursor</link></item><item><title>2025.06.09 【第5回】デジタル化・DX推進展に図面AI・積算AI等を出展いたしました 2025年6月に東京ビッグサイトで開催された「【第5回】デジタル化・DX推進展」に出展いたしました。</title><link>https://renue.co.jp/posts/DX</link></item><item><title>2025.05.26 Forbes JAPAN様に取材頂きました。 ｜株式会社renue 弊社代表の山本より、「若き起業家が提供するCAD生成AIが日本のものづくりに風穴をあける」としてお話をさせて頂いております。</title><link>https://renue.co.jp/posts/Z_Kwijti</link></item><item><title>2025.05.27 【製造・建設業界向け】図面読み取り〜活用を一気通貫「renue 図面管理・活用AIソリューション」提供開始 ｜株式会社renue（リノイ） この記事はPRTimesにて配信されました</title><link>https://renue.co.jp/posts/cZCbiLbd</link></item><item><title>2025.07.14 AIと一緒に作る！農薬散布ドローン自律航行システムの開発体験記 エンジニアのための情報共有コミュニティ「Zenn」にて配信されました。AIの力を借りた、農薬散布ドローンの自律航行システムの開発体験を紹介します。</title><link>https://renue.co.jp/posts/drone</link></item><item><title>2025.04.30 「Japan IT Week 春 2025」に図面AI・積算AIを出展いたしました 2025年4月に東京ビッグサイトで開催された「Japan IT Week 春 2025」に出展いたしました。</title><link>https://renue.co.jp/posts/japan-it-week</link></item><item><title>AI人材育成・常駐派遣</title><link>https://renue.co.jp/services/ai-training</link></item><item><title>図面AI/積算AI</title><link>https://renue.co.jp/services/drawing-ai</link></item><item><title>新規事業・AI開発</title><link>https://renue.co.jp/services/new-business-ai</link></item><item><title>DXæ¨é²</title><link>https://www.mirait-one.com/solution/issue/dx-implementation/</link></item><item><title>èªæ²»ä½DXæ¨é²</title><link>https://www.mirait-one.com/solution/issue/government-dx/</link></item><item><title>æ°´éDX</title><link>https://www.mirait-one.com/solution/issue/water-dx/</link></item><item><title>ãã¸ã¿ã«ãã©ã³ã¹ ãã©ã¼ã¡ã¼ã·ã§ã³ï¼DXï¼</title><link>https://www.mirait-one.com/solution/top-category/dx/</link></item><item><title>Education Introducing Claude for Education Apr 02, 2025</title><link>https://www.anthropic.com/news/introducing-claude-for-education</link></item><item><title>Announcements Anthropic Economic Index: Insights from Claude 3.7 Sonnet Mar 27, 2025</title><link>https://www.anthropic.com/news/anthropic-economic-index-insights-from-claude-sonnet-3-7</link></item><item><title>September 9, 2025 | Sony AI Advancing Analog Design with AI: Sony AI’s Contributions at MLCAD 2025</title><link>https://ai.sony/blog/Advancing-Analog-Design-with-AI-Sony-AI’s-Contributions-at-MLCAD-2025/</link></item><item><title>August 26, 2025 | Game AI Gaming Life at Sony AI Robotics Sony AI Sony AI’s Deep RL Team on Why the Hardest Problems Still Matter</title><link>https://ai.sony/blog/Sony-AI’s-Deep-RL-Team-on-Why-the-Hardest-Problems-Still-Matter/</link></item><item><title>老朽インフラ・BCP・災害対策</title><link>https://www.mirait-one.com/solution/issue/disaster/</link></item><item><title>安心安全な社会インフラ</title><link>https://www.mirait-one.com/solution/top-category/infrastructure/</link></item><item><title>Product Claude can now search the web Mar 20, 2025</title><link>https://www.anthropic.com/news/web-search</link></item><item><title>Announcements Anthropic partners with U.S. National Labs for first 1,000 Scientist AI Jam Feb 28, 2025</title><link>https://www.anthropic.com/news/anthropic-partners-with-u-s-national-labs-for-first-1-000-scientist-ai-jam</link></item><item><title>Announcements Anthropic achieves ISO 42001 certification for responsible AI Jan 13, 2025</title><link>https://www.anthropic.com/news/anthropic-achieves-iso-42001-certification-for-responsible-ai</link></item><item><title>Announcements Lyft to bring Claude to more than 40 million riders and over 1 million drivers Feb 6, 2025</title><link>https://www.anthropic.com/news/lyft-announcement</link></item><item><title>Product Tailor Claude’s responses to your personal style Nov 26, 2024</title><link>https://www.anthropic.com/news/styles</link></item><item><title>Product Claude 3.5 Haiku on AWS Trainium2 and model distillation in Amazon Bedrock Dec 3, 2024</title><link>https://www.anthropic.com/news/trainium2-and-distillation</link></item><item><title>Announcements Powering the next generation of AI development with AWS Nov 22, 2024</title><link>https://www.anthropic.com/news/anthropic-amazon-trainium</link></item><item><title>Announcements Claude 3.5 Sonnet on GitHub Copilot Oct 29, 2024</title><link>https://www.anthropic.com/news/github-copilot</link></item><item><title>Announcements Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku Oct 22, 2024</title><link>https://www.anthropic.com/news/3-5-models-and-computer-use</link></item><item><title>Societal Impacts Anthropic窶冱 response to Governor Newsom窶冱 AI working group draft report Mar 19, 2025</title><link>https://www.anthropic.com/news/anthropic-s-response-to-governor-newsom-s-ai-working-group-draft-report</link></item><item><title>Announcements Anthropic raises Series E at $61.5B post-money valuation Mar 03, 2025</title><link>https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation</link></item><item><title>Societal Impacts Anthropic窶冱 recommendations to OSTP for the U.S. AI action plan Mar 06, 2025</title><link>https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan</link></item><item><title>Announcements Claude 3.7 Sonnet and Claude Code Feb 24, 2025</title><link>https://www.anthropic.com/news/claude-3-7-sonnet</link></item><item><title>Announcements Claude and Alexa+ Feb 26, 2025</title><link>https://www.anthropic.com/news/claude-and-alexa-plus</link></item></channel></rss>